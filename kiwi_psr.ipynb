{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_layer:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError(\"forward is not implemented\")\n",
    "    \n",
    "    def backward(self):\n",
    "        raise NotImplementedError(\"backward is not implemented\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv(base_layer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def convolving(self):\n",
    "        raise NotImplementedError(\"convolving is not implemented\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### psr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class psr_layer(base_layer):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def grader(self):\n",
    "        raise NotImplementedError(\"grader is not implemented\")\n",
    "    \n",
    "    # NOTE: کاست رو یطوری باید بنویسیم که نتیجه غایی رو بدیم بهش و کاست ر بده بهمون\n",
    "    # کاست رو باید توی نتورک بیایم برای نود تعریف کنیم!\n",
    "    def cost(self):\n",
    "        raise NotImplementedError(\"cost is not implemented\")\n",
    "    \n",
    "    # REST OF NET\n",
    "    def ron(self):\n",
    "        raise NotImplementedError(\"rest of net is not implemented\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qconv(conv):\n",
    "    # TODO: add ACTIVATION, STRID and PAD\n",
    "    # if you like DO: iylDO: add upsilon value\n",
    "    def __init__(self, kernel: qml.QNode, kernel_size: int, weight, lr: float = 0.01) -> None:\n",
    "        self.kernel = kernel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.weight = weight\n",
    "        # self.activator = activator\n",
    "        # self.strid = strid\n",
    "        # self.pad = pad\n",
    "        self.lr = lr\n",
    "\n",
    "    # NOTE: این صرفا یه ورودی می‌گیره، دیتاست ر شبکه می‌گیره\n",
    "    def forward(\n",
    "            self, input_data=None, # صرفا موقع فوروارد جدید مقدار داره\n",
    "            input_theta=None, # صرفا موقع پارامتر شیقت رول مقدار داره\n",
    "            input_label=None # صرفا موقع فوروارد جدید مقدار داره\n",
    "                ):\n",
    "        \n",
    "        # NOTE: اگه دیتای ورودی جدیدی نداشته باشیم لست اینپوتو استفاده می‌کنه\n",
    "        if isinstance(input_data, type(None)):\n",
    "        # if input_data  == None:\n",
    "            data = self.last_data\n",
    "        # -NOTE: در غیر این‌صورت لست اینپوتو آپدیت می‌کنه و از ورودی جدید استفاده می‌کنه\n",
    "        else:\n",
    "            data = self.last_data = input_data\n",
    "\n",
    "        if isinstance(input_label, type(None)):\n",
    "            label = self.last_label\n",
    "        else:\n",
    "            label = self.last_label = input_label\n",
    "\n",
    "        # NOTE: اگه تتای جدیدی نباشه از تتای درونی استفاده می‌کنه\n",
    "        # if input_theta == None:\n",
    "        if isinstance(input_theta, type(None)):\n",
    "            theta = self.weight\n",
    "        # -NOTE: در غیر این‌صورت از تتاهای جدید استفاده می‌کنه و <<البته>> تتای درونی رو آپدیت نمیکنه\n",
    "        # چون آپدیت تتا یه پروسه کلا جداست\n",
    "        else:\n",
    "            theta = input_theta\n",
    "        \n",
    "        # WARN: این با یکمدار بعنوان کرنل چند کرنل رو اجرا می‌کنه\n",
    "\n",
    "        xd, yd = data.shape\n",
    "\n",
    "        mxd = xd-(self.kernel_size-1)\n",
    "        myd = yd-(self.kernel_size-1)\n",
    "\n",
    "        output = np.zeros((self.weight.shape[0], mxd, myd))\n",
    "        \n",
    "        for i, j in enumerate(theta):\n",
    "            output[i] = self.convolving(data, j)\n",
    "\n",
    "        output = np.sum(output, 0)\n",
    "\n",
    "        return output.numpy()\n",
    "    \n",
    "    def convolving(self, data, theta):\n",
    "\n",
    "        xd, yd = data.shape\n",
    "\n",
    "        # WARNING: این فقط برای یک قدم یک قدم بدون پدینگ جلو رفتنه\n",
    "        mxd = xd-(self.kernel_size-1)\n",
    "        myd = yd-(self.kernel_size-1)\n",
    "        map = np.zeros((mxd, myd))\n",
    "\n",
    "        for row in range(mxd):\n",
    "            for column in range(myd):\n",
    "                # NOTE: یه تارگت برمی‌داریم\n",
    "                target = data[row: row+self.kernel_size, column: column+self.kernel_size]\n",
    "                res = self.kernel(\n",
    "                                np.pi/(target.reshape(self.kernel_size*self.kernel_size)+1), \n",
    "                                theta\n",
    "                                ).numpy()\n",
    "                map[row, column] = res\n",
    "\n",
    "        return map.numpy()\n",
    "    \n",
    "    # TODO: باید گرادیان حساب کنیم و وزنارو آپدیت کنیم\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError(\"backward is not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class qconv_psr(qconv, psr_layer): # qconv_layer with parameter shift rule\n",
    "\n",
    "    def __init__(self, kernel: qml.QNode, kernel_size: int, weight, lr: float = 0.01) -> None:\n",
    "        super().__init__(kernel, kernel_size, weight, lr)\n",
    "\n",
    "        # TODO: باید گرادیان حساب کنیم و وزنارو آپدیت کنیم\n",
    "    def backward(self, *args):\n",
    "        # WARN: همچنان برای صرف همون فرمت کار میکنه\n",
    "        self.gradient = np.zeros(self.weight.shape)\n",
    "\n",
    "        for i, j in enumerate(self.weight):\n",
    "            self.gradient[i] = self.grader(i, j)\n",
    "\n",
    "        self.gradient = self.gradient.numpy()\n",
    "        self.weight -= self.gradient*self.lr\n",
    "\n",
    "        return args\n",
    "    \n",
    "    def grader(self, index, value):\n",
    "        differ = np.zeros(self.weight.shape)\n",
    "        differ[index] = 0.001*value # -> UPSILON * THETA\n",
    "\n",
    "        up_weights = self.weight+differ\n",
    "        down_weights = self.weight-differ\n",
    "\n",
    "        up_cost = self.cost(\n",
    "                        self.last_label, \n",
    "                        self.ron.res(self.forward(input_theta=up_weights))\n",
    "                        )\n",
    "        down_cost = self.cost(\n",
    "                        self.last_label, \n",
    "                        self.ron.res(self.forward(input_theta=down_weights))\n",
    "                        )\n",
    "        \n",
    "        return (up_cost-down_cost)/(2*0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pooling2d(base_layer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        mxd = ((input_data.shape[0]-self.kernel_size)/2)+1\n",
    "        myd = ((input_data.shape[1]-self.kernel_size)/2)+1\n",
    "        \n",
    "        map = np.zeros(mxd, myd)\n",
    "\n",
    "        for row in range(mxd):\n",
    "            for column in range(myd):\n",
    "                target = input_data[row: row+self.kernel_size, column: column+self.kernel_size]\n",
    "                res = self.pooling(target)\n",
    "                map[row, column] = res\n",
    "\n",
    "    def pooling(self):\n",
    "        raise NotImplementedError(\"pooling is not implemented\")\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pooling2d(pooling2d):\n",
    "    # TODO: add STRID and PAD\n",
    "    # WARN: این فثط برای استرید ۲ کار می‌کنه\n",
    "    def __init__(self, size=2) -> None:\n",
    "        self.kernel_size = size\n",
    "        # self.activator = activator\n",
    "        # self.strid = strid\n",
    "        # self.pad = pad\n",
    "    \n",
    "    def pooling(t):\n",
    "        return np.max(t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense(base_layer):\n",
    "    # TODO: add the feature that make initial weights by the way\n",
    "    def __init__(self, in_dim, out_dim, in_weights, lr = 0.01) -> None:\n",
    "        self.lr = lr\n",
    "        self.weight = weights\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classic_dense:\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, weights, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.weight = weights\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward_pass(self, input_data, *args):\n",
    "        self.last_data = input_data\n",
    "        output = np.matmul(input_data, self.weight)\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def backprop(self, grad_previous):\n",
    "        self.grad = np.matmul((self.X.transpose()), grad_previous)/self.in_dim\n",
    "        self.weight = self.weight - (self.lr*self.grad)\n",
    "        output = np.matmul(grad_previous, self.Theta.transpose())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classic_dense_psr:\n",
    "\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, weights, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.weight = weights\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "    def forward_pass(\n",
    "            self, input_data=None,\n",
    "            input_theta=None,\n",
    "            input_label=None):\n",
    "        \n",
    "        # NOTE: اگه دیتای ورودی جدیدی نداشته باشیم لست اینپوتو استفاده می‌کنه\n",
    "        if isinstance(input_data, type(None)):\n",
    "            data = self.last_data\n",
    "        # -NOTE: در غیر این‌صورت لست اینپوتو آپدیت می‌کنه و از ورودی جدید استفاده می‌کنه\n",
    "        else:\n",
    "            data = self.last_data = input_data\n",
    "\n",
    "        if isinstance(input_label, type(None)):\n",
    "            label = self.last_label\n",
    "        else:\n",
    "            label = self.last_label = input_label\n",
    "\n",
    "        # NOTE: اگه تتای جدیدی نباشه از تتای درونی استفاده می‌کنه\n",
    "        if isinstance(input_theta, type(None)):\n",
    "            theta = self.weight\n",
    "        # -NOTE: در غیر این‌صورت از تتاهای جدید استفاده می‌کنه و <<البته>> تتای درونی رو آپدیت نمیکنه\n",
    "        # چون آپدیت تتا یه پروسه کلا جداست\n",
    "        else:\n",
    "            theta = input_theta\n",
    "\n",
    "        output = np.matmul(data, self.weight)\n",
    "        return output\n",
    "    \n",
    "\n",
    "        # TODO: باید گرادیان حساب کنیم و وزنارو آپدیت کنیم\n",
    "    def backward(self, *args):\n",
    "        # WARN: همچنان برای صرف همون فرمت کار میکنه\n",
    "        self.gradient = np.zeros(self.weight.shape)\n",
    "\n",
    "        for i, j in enumerate(self.weight):\n",
    "            self.gradient[i] = self.grader(i, j)\n",
    "\n",
    "        self.gradient = self.gradient.numpy()\n",
    "        self.weight -= self.gradient*self.lr\n",
    "\n",
    "        return args\n",
    "    \n",
    "    def grader(self, index, value):\n",
    "        differ = np.zeros(self.weight.shape)\n",
    "        differ[index] = 0.001*value # -> UPSILON * THETA\n",
    "\n",
    "        up_weights = self.weight+differ\n",
    "        down_weights = self.weight-differ\n",
    "\n",
    "        up_cost = self.cost(\n",
    "                        self.last_label, \n",
    "                        self.ron.res(self.forward(input_theta=up_weights))\n",
    "                        )\n",
    "        down_cost = self.cost(\n",
    "                        self.last_label, \n",
    "                        self.ron.res(self.forward(input_theta=down_weights))\n",
    "                        )\n",
    "        \n",
    "        return (up_cost-down_cost)/(2*0.001) # > /2*upsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq:\n",
    "\n",
    "    def __init__(self, funcs):\n",
    "        self.funcs = funcs\n",
    "\n",
    "    def res(self, input):\n",
    "        res = input\n",
    "        for i in self.funcs:\n",
    "            res = i.forward(res, res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSR_Net:\n",
    "# WARN: صرفا برای مدارهایی که تمامشون با پارامتر شیقت رول باشه( یا از اول تا یجایی جتما باشه)\n",
    "\n",
    "    # WARN: اینجا م دیتاست نمی‌گیریم و یدوه دیتا میگیریم. دیتاست رو توی ترین میگیریم\n",
    "    def __init__(self, network, cost) -> None :\n",
    "        self.network = network\n",
    "        self.cost = cost\n",
    "        self.predict()\n",
    "\n",
    "    # WARN: این قضیه بهینه نیست و اگه لایه ای از پارامنر شیفت استفاده نکنه کلی کاست و آراوان اضافه ایجاد می‌کنه\n",
    "    def predict(self):\n",
    "        for i, j in enumerate(self.network):\n",
    "            j.cost = self.cost\n",
    "            j.ron = Seq(self.network[i+1:])\n",
    "\n",
    "    # TODO: صحت و کاست و تایم و اینارم برگردون\n",
    "    def train(self, dataset, labelset):\n",
    "        for i, j in zip(dataset, labelset):\n",
    "            self.forward(i, j)\n",
    "            self.backward()\n",
    "\n",
    "    def forward(self, data, label):\n",
    "        res = data\n",
    "        for i in self.network:\n",
    "            res = i.forward(res, label)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def backward(self):\n",
    "        back_g = 0\n",
    "        for i in (reversed(self.Network)):\n",
    "            back_g = i.backward(back_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(target, prediction):\n",
    "    l = np.multiply(target, prediction)\n",
    "    return -np.log(l.max()).numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actv(base_layer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def actv_func(self):\n",
    "        raise NotImplementedError(\"activation function is not implemented\")\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sf(x):\n",
    "    return np.power(np.e, x)\n",
    "\n",
    "class soft_max_1d(actv):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.actv_func(input_data)\n",
    "\n",
    "    def actv_func(self, data):\n",
    "        a = list(map(sf, data))\n",
    "        s = sum(a)\n",
    "        res = []\n",
    "        for i in a:\n",
    "            res.append(i/s)\n",
    "        return res\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm(x):\n",
    "    return (1/(1+np.power(np.e, -x)))\n",
    "\n",
    "class sigmoid(actv):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        return self.actv_func(input_data)\n",
    "\n",
    "    def actv_func(self, data):\n",
    "        return list(map(sm, data))\n",
    "    \n",
    "    def backward(self, *args):\n",
    "        return args"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
